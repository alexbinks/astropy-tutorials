{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d9a5e47",
   "metadata": {},
   "source": [
    "# Dealing with multiple Radial Velocity measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2636c",
   "metadata": {},
   "source": [
    "It is a common task in astronomy to calculate a final measured quantity (and error) when there are multiple individual measurements for a particular object. This is often the case when dealing with radial velocities (RVs) for stars, particularly in the era of large spectroscopic surveys such as RAVE, Gaia DR2 (soon to be DR3) and GALAH.\n",
    "\n",
    "We also know that approximately half of all stars belong in multiple systems, and depending on their orbital properties (e.g., physical separation, eccentricity, inclination), their multiplicity status is given away by the fact that the components vary in RV from as low as 1 to 100 km/s. Whilst obvious temporal changes in RV are a giveaway for identifying multiples, it should be noted there is always some (usually small) probability that multiple systems evade detection because they are observed at the same orbital phase!\n",
    "\n",
    "### With this in mind, the following program:\n",
    "\n",
    "- reads in a table of RV measurements for any set of stars\n",
    "\n",
    "- makes a decision on whether the star is multiple or likely single\n",
    "- calculates a final RV from the individual measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1639b6",
   "metadata": {},
   "source": [
    "### Some points before we begin:\n",
    "\n",
    "1. The table consists of a star identifier (that must be exactly the same if multiple entries are present), a reference to the source catalogue, the RV, and the RV error.\n",
    "\n",
    "2. For the cases where no RV error is calculated (tut, tut), for lack of a better method, we have assumed this error to be twice the median error bar from the whole sample of RV measurements. It is the user's discretion whether or not to include data that are missing errors (but sometimes even these measurements have intrinsic value if, say, there are no other measurements).\n",
    "\n",
    "3. The confidence placed in the reliability of each RV measurement is equal regardless of provenance.\n",
    "\n",
    "4. In the rare case where two RV measurements for a source differ by an amount greater than the physical maximum for an equal mass binary system, $\\delta RV \\geq 437\\times\\frac{\\sqrt{M/M_{\\odot}}}{\\sqrt{a/R_{\\odot}}}$ km/s -- *Bowers & Deeming 1984, 1984astr.book.....B*) then we discard the largest outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458886b8",
   "metadata": {},
   "source": [
    "### Part 1: Reading in the catalogue and pre-cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac145f",
   "metadata": {},
   "source": [
    "Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe48f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import unique, Table\n",
    "from astropy.io import ascii\n",
    "from astropy.stats import sigma_clip\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad9f2af",
   "metadata": {},
   "source": [
    "Read in the input file\n",
    "The minimum requirement is;\n",
    "- name\n",
    "- RV\n",
    "- eRV\n",
    "- reference\n",
    "\n",
    "Currently the following line would read in a basic ascii (space-separated)\n",
    "file, but \"ascii.read\" can take all kinds of formats, which are described\n",
    "in: https://docs.astropy.org/en/stable/io/ascii/read.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f2d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in = ascii.read(\"AllRVs.dat\", format='basic', delimiter=' ', guess=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c598d8",
   "metadata": {},
   "source": [
    "In case of missing errors, use 2x the median error from all observations. In our example missing errors are marked as \"-999\". Please modify this line to make sense of your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d553c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in[\"eRV\"] = np.where(f_in[\"eRV\"] < -99, 2.0*np.median(f_in[\"eRV\"]), f_in[\"eRV\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a31abe",
   "metadata": {},
   "source": [
    "Get a list of unique names for each star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aecd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_un = unique(f_in, keys='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ec938",
   "metadata": {},
   "source": [
    "The maximum possible RV difference between two observations was given earlier (point 4). One may wish to include columns in their input table of estimated mass and radius. In our case we are dealing mainly with  early K-type pre-main sequence stars: mass = $0.8M_{\\odot}$ and radius = $0.7R_{\\odot}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b7eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass, radius = 0.8, 0.7\n",
    "d_max = 437.*np.sqrt(mass)/np.sqrt(radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b10711a",
   "metadata": {},
   "source": [
    "Now we want to remove any entries in the catalogue that cause $\\delta RV \\geq 437\\times\\frac{\\sqrt{M/M_{\\odot}}}{\\sqrt{a/R_{\\odot}}}$ km/s. Note that we only remove the object that we believe is causing the (unphysically) large RV difference. To do so we loop over each unique star name, and if there are $\\geq 2$ measurements, construct an array of pairwise RV differences, for example:\n",
    "- 3 measurements: A vs B, A vs C, B vs C\n",
    "- 4 measurements: A vs B, A vs C, A vs D, B vs C, B vs D, C vs D\n",
    "\n",
    "The arrays are sorted from largest to smallest values in $\\Delta RV$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30713fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in f_un:\n",
    "    g = f_in[f_in[\"name\"] == f[\"name\"]]\n",
    "#if there are 2 or more measurements we want to test how consistent the RVs are.\n",
    "    x = len(g)\n",
    "# Remove stars that cause RV differences larger than d_max.\n",
    "# Get pair-wise RV measurements and find their differences\n",
    "# using \"itertools.combinations\"\n",
    "    if x >= 2:\n",
    "        p = list(itertools.combinations(g[\"RV\"], 2))\n",
    "        b = list((i,j) for ((i,_),(j,_)) in itertools.combinations(enumerate(g[\"RV\"]), 2))\n",
    "        c1, c0, darr = [g[\"eRV\"][c[1]] for c in b], [g[\"eRV\"][c[0]] for c in b], [abs(z[1]-z[0]) for z in p]\n",
    "# for the new pairwise arrays, find the locations where the RV difference > d_max\n",
    "        arr_bad = np.array(b)[np.array(darr) > d_max]\n",
    "# if there are any examples where the RV difference > d_max\n",
    "   # and at least 2 cases where this happens\n",
    "        if np.any(arr_bad) and len(arr_bad) >= 2:\n",
    "            ind_outliers = np.array(arr_bad).ravel()\n",
    "   # find the entry which is causing the big RV difference (occurs the most often)\n",
    "            g_r = g[np.bincount(ind_outliers).argmax()]\n",
    "            f_in_r = [(f_in[\"cat\"] == g_r[1]) & (f_in[\"name\"] == g_r[0])]\n",
    "            f_in_ind = np.where(f_in_r)[1]\n",
    "   # remove this entry...\n",
    "            f_in.remove_row(f_in_ind[0])\n",
    "\n",
    "   # if only one occasion where RV difference is large (i.e., 2 measurements)\n",
    "        if np.any(arr_bad) and len(arr_bad) < 2:\n",
    "            ind_outliers = np.array(arr_bad).ravel()\n",
    "            g_r = g[ind_outliers]\n",
    "   # select the entry with the largest RV error as the outlier to remove\n",
    "            g_max = g[g_r[\"eRV\"] == max(g_r[\"eRV\"])]\n",
    "            f_in_r = [(f_in[\"cat\"] == g_max[\"cat\"]) & (f_in[\"name\"] == g_max[\"name\"])]\n",
    "            f_in_ind = np.where(f_in_r)[1]\n",
    "            f_in.remove_row(f_in_ind[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73400815",
   "metadata": {},
   "source": [
    "### Part 2: The decision process for multiple/single stars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca8ff2",
   "metadata": {},
   "source": [
    "The next job is to combine multiple RV measurements again (this time without any extreme RV outliers), and create an array of pairwise RV differences. Sort the array in descending values of RV difference. If the RV difference is larger than the following criteria:\n",
    "\n",
    "$\\Delta RV > \\zeta\\times max(\\sigma_{RV_{1}}, \\sigma_{RV_{2}})$, where $\\zeta = 3.0$ (but can be altered).\n",
    "\n",
    "Then we flag the star as a likely binary. If this criteria is not matched, move to the next lowest RV difference and recalculate. If, at the end this still isn't matched, then flag the star as a probable single."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsig = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e32131",
   "metadata": {},
   "source": [
    "Create an empty array \"F_arr\", which will store the designated multiplicity status, where:\n",
    "- F_arr = 5: multiple, at least one $\\Delta RV > \\zeta\\times max(\\sigma_{RV_{1}}, \\sigma_{RV_{2}})$\n",
    "- F_arr = 3: (probably) single, every $\\Delta RV \\leq \\zeta\\times max(\\sigma_{RV_{1}}, \\sigma_{RV_{2}})$\n",
    "- F_arr = 1: = only 1 RV measurement, we can't say anything about multiplicity status\n",
    "- F_arr = 0: = no RV measurements available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_un = unique(f_in, keys='name')\n",
    "F_Arr = []\n",
    "\n",
    "for f in f_un:\n",
    "    g = f_in[f_in[\"name\"] == f[\"name\"]]\n",
    "#if there are 2 or more measurements we want to test how consistent the RVs are.\n",
    "    x = len(g)\n",
    "\n",
    "    if x >= 2:\n",
    "        p = list(itertools.combinations(g[\"RV\"], 2))\n",
    "        b = list((i,j) for ((i,_),(j,_)) in itertools.combinations(enumerate(g[\"RV\"]), 2))\n",
    "        c1, c0, darr = [g[\"eRV\"][c[1]] for c in b], [g[\"eRV\"][c[0]] for c in b], [abs(z[1]-z[0]) for z in p]\n",
    "        z = np.argsort(darr)\n",
    "        RVd_arr = [[darr[z[i]], c1[z[i]], c0[z[i]]] for i in z]# if darr[i] < d_max]\n",
    "            \n",
    "# fill out the RV flag arrays based on the RV differences (and errors)\n",
    "# 5 = multiple, dRV > d_max\n",
    "# 3 = single, dRV <= d_max\n",
    "# 1 = only 1 RV\n",
    "# 0 = no RVs\n",
    "        for j in range(len(RVd_arr)):\n",
    "            eRV_max = max(RVd_arr[j][2], RVd_arr[j][1])\n",
    "            if RVd_arr == None:\n",
    "                F_Arr.append(0)\n",
    "                break\n",
    "            if len(z) > 1:\n",
    "# if at any point |RV| > N*max(eRV) then we have no choice\n",
    "# but to flag it as a probable multiple.\n",
    "                if (RVd_arr[j][0] > Nsig*eRV_max):\n",
    "                    F_Arr.append(5)\n",
    "                    break\n",
    "# |RV| <= N*max(eRV)\n",
    "# could be single, check the next RV difference\n",
    "                if (RVd_arr[j][0] <= Nsig*eRV_max):\n",
    "                    j=j+1\n",
    "# if after all the iterations we get here and we still didn't see any\n",
    "# evidence of binarity, flag it as a single star.\n",
    "                    if j == len(z)-1:\n",
    "                         F_Arr.append(3)\n",
    "                         break\n",
    "            if len(z) == 1:\n",
    "                if (RVd_arr[j][0] > Nsig*eRV_max):\n",
    "                    F_Arr.append(5)\n",
    "#                    break\n",
    "                if (RVd_arr[j][0] <= Nsig*eRV_max):\n",
    "                    F_Arr.append(3)\n",
    "#                    break\n",
    "\n",
    "    if x == 1:\n",
    "        if g[\"RV\"][0] > -900:\n",
    "            F_Arr.append(1)\n",
    "        else:\n",
    "            F_Arr.append(0)\n",
    "    if x == 0:\n",
    "        F_Arr.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3251216",
   "metadata": {},
   "source": [
    "### Part 3: obtaining a final RV measurement\n",
    "Regardless of the multiplicity status assigned in Part 2, we now calculate a final RV measurement, and flag \"how\" we measure RV using a measurement flag (Mflag).\n",
    "\n",
    "#### If the star is a probable single, there are two paths to calculating a final RV.\n",
    "\n",
    "1. If the standard deviation in the error bars is less than the standard deviation in the measurements, then take the inverse weighted mean, using the square of the error bars as weights. Mflag = 2.\n",
    "2. Otherwise, adopt the final RV as the value corresponding to the lowest error bar. Mflag = 3.\n",
    "\n",
    "The associtated error in the weighted mean is found using equation (2) and (3) at this link: http://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf\n",
    "\n",
    "#### If the star is likely to be a multiple, use path 1 described above.\n",
    "Mflag = 4.\n",
    "\n",
    "The cases with only 1 or 0 RV measurements is trivial, Mflag = 1 and 0, respectively.\n",
    "\n",
    "Two error bars are provided:\n",
    "- \"sRV\", the error in the weighted mean.\n",
    "- \"eRV\", the average value of the individual error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3367b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "RV_f, eRV_f, sRV_f, RV_T = [], [], [], []\n",
    "\n",
    "for k, l in enumerate(f_un):\n",
    "    g = f_in[f_in[\"name\"] == l[\"name\"]]\n",
    "    sdRV  = np.std(g[\"RV\"])\n",
    "    sdeRV = np.std(g[\"eRV\"])\n",
    "#if there are 2 or more measurements we want to test how consistent the RVs are.\n",
    "    x = len(g)\n",
    "    if x >= 2:\n",
    "# if we don't class the star as a possible binary\n",
    "        w = np.array(1.0/(g[\"eRV\"]**2))\n",
    "        if F_Arr[k] != 5:\n",
    "# if the std in the errors is less than the std in the mean RV\n",
    "# take the weighted average and error\n",
    "\n",
    "# weighted error is found like this:\n",
    "# http://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf\n",
    "\n",
    "            if sdeRV <= sdRV:\n",
    "                w_RV = np.average(g[\"RV\"], weights=w)\n",
    "                val = np.array(g[\"RV\"]**2)\n",
    "                var = ((np.sum(w*val)/np.sum(w))-w_RV**2) * (x/(x-1))\n",
    "                RV_f.append(w_RV)\n",
    "                sRV_f.append(np.sqrt(var/x))\n",
    "                eRV_f.append(np.average(g[\"eRV\"]))\n",
    "                RV_T.append(2)\n",
    "            else:\n",
    "                RV_f.append(g[\"RV\"][np.argsort(g[\"eRV\"][0])][0])\n",
    "                eRV_f.append(g[\"eRV\"][np.argsort(g[\"eRV\"][0])][0])\n",
    "                sRV_f.append(np.average(g[\"eRV\"]))\n",
    "                RV_T.append(3)\n",
    "        if F_Arr[k] == 5:\n",
    "            w_RV = np.average(g[\"RV\"], weights=w)\n",
    "            val = np.array(g[\"RV\"]**2)\n",
    "            var = ((np.sum(w*val)/np.sum(w))-w_RV**2) * (x/(x-1))\n",
    "            RV_f.append(np.average(g[\"RV\"], weights=w))\n",
    "            sRV_f.append(np.sqrt(var/x))\n",
    "            eRV_f.append(np.average(g[\"eRV\"]))\n",
    "            RV_T.append(4)\n",
    "    if x == 1:\n",
    "        RV_f.append(g[\"RV\"][0])\n",
    "        sRV_f.append(0.0)\n",
    "        eRV_f.append(g[\"eRV\"][0])\n",
    "        RV_T.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2581278",
   "metadata": {},
   "source": [
    "Finally, print all these results to a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a902d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = [f_un[\"name\"], RV_f, sRV_f, eRV_f, RV_T]\n",
    "\n",
    "f_table = Table(data=f_out, masked=False, names=('Name','RVfin','sRV','eRV','RV_T'))\n",
    "\n",
    "f_table[\"Name\"].format = '%s'\n",
    "f_table[\"RVfin\"].format = '%+9.2f'\n",
    "f_table[\"sRV\"].format = '%9.2f'\n",
    "f_table[\"eRV\"].format = '%9.2f'\n",
    "f_table[\"RV_T\"].format = '%i'\n",
    "\n",
    "ascii.write(f_table, \"finalRVs.csv\", format='csv', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
